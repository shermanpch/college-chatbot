# Data Scripts

This directory contains scripts for university data collection and processing workflows.

## ğŸ“ Directory Structure

### ğŸ“¥ [scraper/](scraper/)
**Data Collection & Web Scraping**
- **Peterson University Data**: Complete pipeline for scraping and processing university information from Peterson's website
- Automated data discovery, batch processing, validation, and error recovery
- Support for selective downloads, URL validation, and comprehensive logging

## ğŸš€ Quick Start Guide

### Data Collection

**Process Peterson university data (complete pipeline):**
```bash
# 1. Extract URLs
python data/scripts/scraper/peterson_search_data/001_get_peterson_urls.py

# 2. Get correct URLs
python data/scripts/scraper/peterson_search_data/002_get_correct_peterson_url.py

# 3. Batch scrape data
python data/scripts/scraper/peterson_search_data/003_get_peterson_data.py --num-batches 10

# 4. Re-scrape failed URLs (if needed)
python data/scripts/scraper/peterson_search_data/004_rescrape_failed_urls.py

# 5. Scrape course information using BeautifulSoup
python data/scripts/scraper/peterson_search_data/005_scrape_courses_bs.py

# 6. Combine course data
python data/scripts/scraper/peterson_search_data/006_combine_peterson_courses.py

# 7. Clean and combine data
python data/scripts/scraper/peterson_search_data/007_clean_peterson_data.py
```

## ğŸ“¦ Installation

Install dependencies for data scraping:

```bash
pip install -r data/scripts/scraper/requirements.txt
```

## ğŸ“Š Data Organization

### Input Data Structure
```
data/
â””â”€â”€ cleaned/           # Processed and cleaned datasets
```

### Output Data Structure
```
data/
â”œâ”€â”€ external/
â”‚   â”œâ”€â”€ peterson_data/
â”‚   â”‚   â””â”€â”€ *.json                    # Raw scraped university data
â”‚   â””â”€â”€ peterson_courses_data/
â”‚       â””â”€â”€ *.json                    # Course data files
â””â”€â”€ cleaned/
    â”œâ”€â”€ peterson_university_urls.json           # University search results
    â”œâ”€â”€ peterson_university_urls_backup.json    # Backup of university URLs
    â”œâ”€â”€ peterson_url_validation_results.json    # URL validation results
    â”œâ”€â”€ peterson_url_validation_results_backup.json # Backup of validation results
    â””â”€â”€ peterson_data.json                      # Final cleaned university dataset
```

## ğŸ”§ Advanced Usage

### Peterson University Scraper
- **Complete pipeline**: URL discovery, validation, batch scraping, and data cleaning
- **Batch processing**: Efficient parallel scraping using Firecrawl API
- **URL validation**: Smart matching against existing university datasets
- **Error recovery**: Automatic re-scraping of failed URLs
- **Structured data**: Comprehensive university information extraction

## ğŸ“ Logging & Monitoring

All scripts provide comprehensive logging in the `logs/` directory:
- **Peterson scraper**: `logs/peterson_scraper.log`

## ğŸ› ï¸ Troubleshooting

### Common Issues
1. **Network errors**: Check internet connection; scrapers have retry logic
2. **File permissions**: Ensure write access to output directories
3. **Encoding errors**: Scripts include automatic encoding fallback

### Performance Tips
1. **Batch processing**: Download/process in smaller batches for large datasets
2. **Parallel processing**: Scripts use multiprocessing where applicable
3. **Resume capability**: Re-run scripts to continue interrupted operations

## ğŸ“š Detailed Documentation

For comprehensive documentation on the scraper component, see:
- [Scraper Documentation](scraper/README.md)

## ğŸ¯ Typical Workflow

1. **Collect Data**: Use Peterson scraper to download university data
2. **Process & Analyze**: Use the scraped data for analysis and model development
3. **Scale Up**: Apply final processing to full datasets when ready
